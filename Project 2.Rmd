---
title: "Project 2: H-1B Visa"
author: "Spencer Stucky, Cheng Zeng, Wenyu Zeng, Chao Zhou"
date: "11/20/2019"
output:
  html_document:
    toc: yes
    toc_depth: 4
    fig.align: "center"
    toc_float: yes
    theme: readable
  pdf_document:
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
knitr::opts_chunk$set(include = T)
# knitr::opts_chunk$set(echo = TRUE)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

```{r basicfcn, include=F}
# can add quietly=T option to the require() function
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```

### Import Dataset
```{r, include=T}
house <- read.csv("kc_house_data.csv")
summary(house)
house$view <- as.factor(house$view)
str(house)
```

### Chapter 1 SMART Question


### Logistic Regression
#### Full Model
```{r}
house$waterfront <- as.factor(house$waterfront)
house$condition <- as.factor(house$condition)
house$grade <- as.factor(house$grade)
house$yr_renovated[house$yr_renovated > 0] <- 1
house$yr_renovated <- as.factor(house$yr_renovated)
head(house)
testlog <- glm(view~price + bedrooms + bathrooms + sqft_living + sqft_lot + waterfront + condition + grade + sqft_above + sqft_basement + yr_built + yr_renovated + sqft_living15 + sqft_lot15 + lat + long + floors, data = house, family = "binomial")
summary(testlog)
```

#### Adjusted Model
```{r, echo=F}
testlog2 <- glm(view~price + bedrooms + bathrooms + sqft_living + sqft_lot + sqft_above + sqft_living15 + yr_built + lat + long, data = house, family = "binomial")
summary(testlog2)
```

### Model Evaluation

#### Full model 
#####ROC & AUC
```{r roc_auc1, echo=F}
loadPkg("pROC") 
prob=predict(testlog , type = c("response"))
house$prob=prob
h <- roc(view~prob, data=house)
auc(h)
plot(h)
```

##### McFadden
```{r, echo=FALSE}
loadPkg("pscl") # use pR2( ) function to calculate McFadden statistics for model eval
admitLogitpr2 = pR2(testlog)
admitLogitpr2
```

#### Adjusted Model
##### ROC & AUC
```{r testlog2, echo=F}
prob=predict(testlog2 , type = c("response"))
house$prob=prob
h <- roc(view~prob, data=house)
auc(h)
plot(h)
```

##### McFadden

```{r McFadden, echo=FALSE}
loadPkg("pscl") # use pR2( ) function to calculate McFadden statistics for model eval
admitLogitpr2 = pR2(testlog2)
admitLogitpr2
```

### Linear Regression
#### Full Model
```{r}

testlm <- lm(price ~view + bedrooms + bathrooms + sqft_living + sqft_lot + waterfront + condition + grade + sqft_above + sqft_basement + yr_built + yr_renovated + sqft_living15 + sqft_lot15 + lat + long + floors + zipcode, data = house)
summary(testlm)
```

#### Adjusted Linear Model
```{r}
testlm2 <- lm(price ~ view + bedrooms + bathrooms + sqft_living + sqft_lot + waterfront + condition + grade + yr_built + yr_renovated + sqft_living15 + sqft_lot15 + lat + long + floors + zipcode, data = house)
summary(testlm2)
```


### K-fold Cross Validation
#### Spencer model 1
```{r, warning=F}
loadPkg("caret")
# Define training control
set.seed(123) 
train.control <- trainControl(method = "cv", number = 10)

model <- train(price ~ grade + view + condition + waterfront, data = house, method = "lm", trControl = train.control)
# Summarize the results
print(model)
```

#### Spencer model 2
```{r,warning=F}
set.seed(123)
model2 <- train(price ~ sqft_living + sqft_lot + sqft_above + sqft_basement, data = house, method = "lm",
               trControl = train.control)
# Summarize the results
print(model2)
```

#### Spencer model 3
```{r,warning=F}
set.seed(123)
model3 <- train(price ~ yr_built + yr_renovated + zipcode, data = house, method = "lm",
               trControl = train.control)
# Summarize the results
print(model3)
```

#### Spencer model 4
```{r,warning=F}
set.seed(123)
model4 <- train(price ~ bedrooms + bathrooms + floors + grade + view + condition + waterfront + sqft_living + sqft_lot + sqft_above + yr_built + yr_renovated + zipcode, data = house, method = "lm",
               trControl = train.control)
# Summarize the results
print(model4)
```

#### Full Model
```{r, warning=F}
set.seed(123)
# Train the model
model5 <- train(price ~view + bedrooms + bathrooms + sqft_living + sqft_lot + waterfront + condition + grade + sqft_above + sqft_basement + yr_built + yr_renovated + sqft_living15 + sqft_lot15 + lat + long + floors + zipcode, data = house, method = "lm",
               trControl = train.control)
# Summarize the results
print(model5)
```

#### Feature Selection based on the full model
```{r, warning=F}
set.seed(123)
model6 <- train(price ~ view + bedrooms + bathrooms + sqft_living + sqft_lot + waterfront + condition + grade + yr_built + yr_renovated + sqft_living15 + sqft_lot15 + lat + long + floors + zipcode, data = house, method = "lm",trControl = train.control)
# Summarize the results
print(model6)
```

#### Table for Cross Validation
```{r}
modelcom <- matrix(c(236009,0.586,153783, 260958,0.494,173416, 362133,0.024,231985, 208763,0.675,135348, 191991,0.726,119763, 191973,0.726,119731),ncol=3,byrow=TRUE)
options("scipen" = 100, "digits" = 2)
colnames(modelcom) <- c("RMSE","RSquared","MAE")
rownames(modelcom) <- c("Model 1","Model 2","Model 3", "Model 4", "Full Model", "Significant feature Model")
modelcom <- as.table(modelcom)
modelcom
```



### KNN
```{r}
set.seed(1)
house_data_train_rows = sample(1:nrow(house),     #<- from 1 to the number of 
                                                     #   rows in the data set
                              round(0.8 * nrow(house), 0),  #<- multiply the 
                                                                #   number of rows
                                                                #   by 0.8 and round
                                                                #   the decimals
                              replace = FALSE)       #<- don't replace the numbers

# Let's check to make sure we have 80% of the rows. 
length(house_data_train_rows) / nrow(house)

house_data_train = house[house_data_train_rows, ]  #<- select the rows identified in
                                                     #   the bank_data_train_rows data
house_data_test = house[-house_data_train_rows, ]  #<- select the rows that weren't
                                                     #   identified in the
                                                     #   bank_data_train_rows data

# Check the number of rows in each set.
head(house_data_train)
nrow(house_data_test)

```
Train the classifier

```{r}
# Let's train the classifier for k = 3. 
# Install the "class" package that we'll use to run kNN.
# Take some time to learn about all its functionality.
# install.packages("class") 
loadPkg("class")

# k-Nearest Neighbor is a randomized algorithm, so make sure to
# use set.seed() to make your results repeatable.
set.seed(1)
house_3NN = knn(train = house_data_train[, c("price", "bedrooms", "bathrooms", "sqft_living", "sqft_lot", "sqft_above", "sqft_living15", "yr_built", "lat", "long")],  #<- training set cases
               test = house_data_test[, c("price", "bedrooms", "bathrooms", "sqft_living", "sqft_lot", "sqft_above", "sqft_living15", "yr_built", "lat", "long")],    #<- test set cases
               cl = house_data_train[, "view"],                         #<- category for true classification
               k = 3) #,                                                    #<- number of neighbors considered
               # use.all = TRUE)                                            #<- control ties between class assignments
                                                                            #   If true, all distances equal to the kth 
                                                                            #   largest are included

# View the output.
str(house_3NN)
length(house_3NN)
table(house_3NN)
```


```{r}
# How does the kNN classification compare to the true class?
# Let's take a look at the confusion matrix by combining the 
# predictions from bank_3NN to the original data set.
kNN_res = table(house_3NN,
                house_data_test$`view`)
kNN_res
sum(kNN_res)  #<- the total is all the test examples

# Select the true positives and true negatives by selecting
# only the cells where the row and column names are the same.
kNN_res[row(kNN_res) == col(kNN_res)]

# Calculate the accuracy rate by dividing the correct classifications
# by the total number of classifications.
kNN_acc = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)
kNN_acc
```

### K-MEAN
```{r, warning=F}
loadPkg("dplyr")
input <- house %>% select("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "sqft_above", "sqft_living15", "yr_built", "lat", "long")
kmeans(input, centers = 3, nstart = 20)
```
```{r}
wssplot <- function(data, nc=15, seed=123){
               wss <- (nrow(data)-1)*sum(apply(data,2,var))
               for (i in 2:nc){
                    set.seed(seed)
                    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
                plot(1:nc, wss, type="b", xlab="Number of groups",
                     ylab="Sum of squares within a group")}

wssplot(input, nc = 20)
```

```{r}
k2 <- kmeans(input, centers = 4, nstart = 20)
```

```{r}
loadPkg("factoextra")
fviz_cluster(k2, data = input)
```

