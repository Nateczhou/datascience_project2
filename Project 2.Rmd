---
title: "Project 2: House Sales in King County, USA"
author: "Spencer Stucky, Cheng Zeng, Wenyu Zeng, Chao Zhou"
date: "11/20/2019"
output:
  html_document:
    toc: yes
    toc_depth: 4
    fig.align: "center"
    toc_float: yes
    theme: readable
  pdf_document:
    toc: yes
    toc_depth: '4'
---

```{r setup, include=FALSE}
# include=T, eval=T, echo=T, results='hide'/'asis',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
knitr::opts_chunk$set(echo = F, warning = F)
# knitr::opts_chunk$set(echo = TRUE)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
```

```{r basicfcn, include=F}
# can add quietly=T option to the require() function
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```



# Chapter 1: Introduction
 
King County locates in Washington state, U.S. It is the most populous county in the state and the 12th-most populous in the country.  (Wikipidia.com) Therefore, this project we are using the data from Kaggle about King County’s house sale. This dataset was provided by King County. It includes the house sold in 2014 and 2015. In this project, we will analyze what features relate to the price of house; and we will try to predict house price. 

# Chapter 2: Description of Data

```{r, include=F}
house <- read.csv('kc_house_data.csv')
head(house)
str(house)
# # house <- na.omit(house) 
```

```{r, include=F}
loadPkg("dplyr")
loadPkg("tidyr")
loadPkg("corrplot")
house$yr_renovated[house$yr_renovated>0] <- 1
```

```{r , echo=FALSE}
#change variables to fator level
house$view <- as.factor(house$view)
house$floors <-as.factor(house$floors)
house$grade <- as.factor(house$grade)
house$waterfront  <- as.factor(house$waterfront )
house$condition  <- as.factor(house$condition )
house$yr_renovated <- as.factor(house$yr_renovated)
house$zipcode <- as.factor(house$zipcode)
house$bedrooms <- as.factor(house$bedrooms)
house$bathrooms <- as.factor(house$bathrooms)
house$lat <- as.factor(house$lat)
house$long <- as.factor(house$long)
summary(house)

```

```{r excludepirce, echo=FALSE}
house = house[house$price <= 2000000,]
hist(house$price)
```

There are 21 variables in this dataset which include all house features in it, such as price, number of bedrooms and bathrooms, dimension, floors, view, grade, condition and years. The view shows from 0 to 4 how good the view of the property, and grade is an index from 1 to 13 to score the building construction and design. After data observation, we decided to exclude the house price that larger than $2million, because this range of house price only takes less than 1% of total house price. Moreover, the original data only shows the year of renovation; thus, we changed the year to 1 if the houses have been renovated and 0 for the houses that have never been renovated. In addition, we change several variables to factors, they are bedrooms, bathrooms, waterfront, view, condition, grade, zip code, latitude and longitude.

```{r str, echo=FALSE}
str(house)
```

Above is the structure of our dataset after changes. From this structure display we can see there are 70 different zip code, 5034 latitude and 752 longitude in this dataset. Thereby, we won’t include these three factors in our further model and analysis. 

## EDA

```{r eda, echo=FALSE}
# change to histogram 
boxplot(price ~ bedrooms, main="Price vs Bedrooms", data=house, col = rainbow(12))
boxplot(price ~ bathrooms, main="Price vs Bathroom", data=house, col = rainbow(12))
boxplot(price ~ floors, main="Price vs Floors", data=house, col = rainbow(12))
boxplot(price ~ waterfront, main="Price vs Waterfront", data=house, col = rainbow(12))
boxplot(price ~ view, main="Price vs View", data=house, col = rainbow(12))
boxplot(price ~ condition, main="Price vs Condition", data=house, col = rainbow(12))
boxplot(price ~ grade, main="Price vs Grade", data=house, col = rainbow(12))
boxplot(price ~ yr_renovated, main="Price vs Renovated", data=house, col = rainbow(12))
```

From the boxplot we run for the 8 factors can see, the 33 bedrooms is not the most expensive house and more people prefer 3 to 4 bedrooms style. The house has waterfront are more expensive than the one doesn’t have, and more houses that have waterfront were sold. Additionally, combine with condition plot; there are 13 grades for the house condition and design, but only few are grade to 1 and 13. Half of the house have fair condition and design; at the meantime, better condition and design worth more.

# Chapter 3 

## S.M.A.R.T. Question

What features are associated with housing prices for King County, WA?

## ANOVA Test

In order to check which variable that associated with housing price, we are using anova test. Before we run the test, our hull hypothesis is price of house with different features are equal.
```{r corr, echo=FALSE}
# plot(price ~ view + bedrooms + bathrooms + sqft_living + sqft_lot + floors + waterfront + condition + grade + sqft_above + 
#                 sqft_basement + yr_built + yr_renovated + sqft_living15 + sqft_lot15+ lat + long, data=house)
houseaov= aov(price ~ bedrooms + bathrooms + sqft_living + sqft_lot + floors + waterfront + view + condition + grade + sqft_above + 
                sqft_basement + yr_built + yr_renovated + sqft_living15 + sqft_lot15, data=house)
summary(houseaov)
```

The above result is our anova test result. From the chart, we can see the p-value of all variables are smaller than 0.05 which means it statistically significant. Therefore, we reject the null hypothesis. The prices of houses with different features are not equal.
As the result, we will use all these 15 variables in the further model building. 


# Chapter 4 Linear Models
## Introduction to Linear Models

In this section we wanted to look at which housing features most affected housing prices in King County, WA. From our analysis, we wanted to be able to build a model that most accurately could describe and hence predict housing sales in the county. Each model is a linear regression on the price at which the house sold. 

Each model takes a different approach to explaining house prices. The first looks at basic categorical variables that desrcibe the house or its appearance using grade, view, condition, waterfront, bedroom, and bathroom variables. The idea here was to exlcude numeric variables such as square footage and the variables desrcibing surrounding houses to just focus on the more simplistic aspects of the house itself.

The second model focuses on the numeric values of square feet for both the house itself and the surrounding 15 houses. The "15" variables are the average square feet of the surrounding 15 houses, for both living space and lot space, to give an idea of what the neighborhood housing size is like. The third model looks at year built and renovated to see if it effects housing price. As it turns out, as the analysis explains below, this was a poor model for explaining the results of our target variable. 

From here, we chose a backward-step selection process for input variables. Model 4 uses all variables to test how all of them effect housing price. Then, we wanted to adjust the model to get the highest R2 with most significant variables. More importantly, we needed to remove some variables that either didn't make sense with our model analysis or they displayed high multicollinearity. We then move to model 5, which utilized backward step selection to remove some variables. Specifically, it tested all relevant variables except for square feet above and square feet basement because they were multicollinear. As well, this model excluded zip code, latitude, and longitude because they did not work for our model and couldn't explain price well because they were difficult to measure. This was the best model we reached for explaining our target variable, price, with all relevant variables. 

## Correlation Plots 

```{r corrplot, results='markup', collapse=F}
house$yr_renovated <- as.numeric(house$yr_renovated)

housecat <- subset(house, select=c(sqft_lot, sqft_living, sqft_above, sqft_basement, sqft_living15, sqft_lot15, yr_renovated, yr_built, price))
str(housecat)

corhousecat = cor(housecat)
corhousecat
corrplot.mixed(corhousecat)

house$yr_renovated <- as.factor(house$yr_renovated)
```

## Linear Models

### Model 1

```{r model 1, results='markup', collapse=F}
loadPkg("faraway")
#model for categorical variables that describe house, exterior factors, and ranking
model_1 <- lm(price ~ grade + view + condition + waterfront + bedrooms + bathrooms, data=house)
summary(model_1)
coef(model_1)
confint(model_1)
vif(model_1)
plot(model_1)
```

In this model, we first wanted to look at the categorical variables for house, exterior factors, and ranking. R2 is 58% thus model explains 58% of target variable. Upper levels of grade, bedroom, bathroom, and condition are signficant, as well as all factor levels of view and waterfront. Categorical variables of grade, view, condition, bedrooms, and waterfront are positively correlated with price, except for condition 2 and 3 and quite a few number of bathroom levels, which are negatively correlated with price.

### Model 2

```{r model 2, results='markup', collapse=F}
#model on distance and size of house in sq ft and neighboring house sizes using numeric variables exluding sqft basement
model_2 <- lm(price ~ sqft_lot + sqft_living + sqft_above + sqft_living15 + sqft_lot15, data=house)
summary(model_2)
coef(model_2)
confint(model_2)
vif(model_2)
plot(model_2)
```

Idea behind this model was to look at sq footage and size of house and surrounding neighborhood to determine if they effect price in some way. Also to look at how these sq ft variables may be collinear and relate to one another. Sqft basement produced NA values and was removed from model. Some moderately high VIFs for sqft living and sqft above - most likely some collinearity there. R2 is 48% thus model explains 48% of results seen in data. All p values are significant. Sq ft lot, sqft living, and sq ft living 15 are positively correlated with price while sqft above and sqft lot 15 are negatively correlated with price. 

### Model 3

```{r model 3, results='markup', collapse=F}
#head(house)
#model on the time the house was built and renovated
#omit zipcode b/c too many zip locations
model_3 <- lm(price ~ yr_built + yr_renovated, data=house)
summary(model_3)
coef(model_3)
confint(model_3)
vif(model_3)
plot(model_3)
```

Idea behind this model was to look at yr built and yr renovated of housing to see if they determined something about price. All are highly signficant at .001 level. Year built and Yr renovated were positively correlated with price, with yr built increasing price by 940. R2 is very low at 2.2% thus model does not do a good job of explaining results in data. 

### Model 4

```{r model 4, results='markup', collapse=F}
#model with all variables but omit zip, long, and lat b/c not relevant to our model's analysis
model_4 <- lm(price ~ bedrooms + bathrooms + floors + grade + view + condition + waterfront + sqft_living + sqft_lot + sqft_above + sqft_living15 + sqft_lot15 + yr_built + yr_renovated + sqft_basement, data=house)
summary(model_4)
coef(model_4)
confint(model_4)
vif(model_4)
plot(model_4)
```

This model used all variables. R2 is 66%. P values are significant except for bedrooms. Some bedroom factor levels, sqft above, and yr built are negatively correlated with price. Bathrooms, floors, grade, view, condition, waterfront, sq ft living, sq ft living of neighborhood houses, and yr renovated are all positively correlated with price and are signficiant. High VIFs for sqft living and sqft above indicate multicollinearity. Sq ft basement produced NA values, unable to identify why but possibly because of relation to another variable or because of many 0 values it had because this didnt happen in a previous model. This model has a good, decently high R2 but is not the best fitting because of multicollinearity and some odd insignificant variables like bedrooms. As well, zipcode, longitude, and latitude do not tell us anything about price given that they are not factored and it is not helpful or plausible to do so. As well, there is multicollinearity for sq ft variables and that should be manipulated in next model. 

### Model 5

```{r model 5, results='markup', collapse=F}
#head(house)
#model with relevant variables but removed sqft above and sq ft basement because of multicollinearity
#Omit zipcode, latitude, and longitude b/c not relevant to model analysis
model_5 <- lm(price ~ view + bedrooms + bathrooms + sqft_living + sqft_lot + waterfront + condition + grade + yr_built + yr_renovated + sqft_living15 + sqft_lot15 + floors, data=house)
summary(model_5)
coef(model_5)
confint(model_5)
vif(model_5)
plot(model_5)
```

R2 is about 66% thus model explains 66% of results in data. Removed sqft above because of moderately high VIF at 7.1. A moderately high VIF at 5 for sqft living - not high enough for concern. Square feet living dropped from 8 to 5 when removing Sq ft above. I took out sqft basement b/c of high VIF value over 20. Upper ends and scores of condition, grade, and bathrooms were significant but not lower levels. Square feet living, square feet lot, waterfront, year built, year renovated, Sq feet living and lot of surrounding 15 houses average, and all factor levels of floor and view were statistically significant. Bedrooms, however, were consistently not significant. Bedrooms are possibly insignificant because overall square foot has more effect than number of bedrooms or there is multicollinearity present. A few notable results from the model: as bedrooms increase price tends to decrease, yr built is also negatively correlated with price, as is sq ft of surrounding lots.We also removed zip code, latitude, and longitude becasue they weren't properly measurable and didn't explain price well. Although this model has a lower R2 than the model above with all the variables, it makes more sense for analytic purposes and explaining the target variable.

# ANOVA for Regression Models

```{r modelanova, results='markup', collapse=F}
anova(model_1, model_2, model_3, model_4, model_5)
```

ANOVA shows us that models 2, 3, 4, and 5 are statistically significant and are good models for explaining the data/target variable. However, we know that the models might overfit the data, thus, we will do PCA/PCR and model evaluations to check if we can have the best model among them.



# Chapter 5 PCA/PCR

After using we trying to use linear model to predict the price in our dataset, we find out there are too many variables for our feature variables. We immediately think of PCA so that we can reduce the dimensionality of our feature variables. Since PCA only works on numerical variable, we decided to pick out all the numerical variables for running PCA. The speed is immediatly improved. We know PCA can also have the effects on saving data storage although in our case the size of this dataset is no way around huge. However, we find it is so fast to run PCA compared with using linear models. We decide to run PCA both with factor variables and without factor variables.

### PCA/PCR Without Factor
```{r PCA_without_factor}
#subset the needed numeric value
house_pca <- subset(house, select = c(price, sqft_living, sqft_lot, sqft_above, sqft_basement, sqft_living15, sqft_lot15))
#do pca
pc_scale <- prcomp(house_pca , scale =TRUE)
pc_noscale <- prcomp(house_pca , scale =FALSE)
summary(pc_scale)
summary(pc_noscale)
# pc_noscale
biplot(pc_scale, scale = 0)
biplot(pc_noscale, scale = 0)
```
  
First we only choose numerical variables when run PCA, since the variance is not too huge between these variables we consider running PCA with both scaled data and unscaled data. Since our y variable is also numeric, we cannot see too much from the biplot. We cannot see obvious clustering of data since all the data are clustered together. However we did see normalizing the variable before running make the variance more equal.

```{r}
#Let us plot the cumulation of variance using the sd
pc_scale_var <- (pc_scale$sdev^2)
pve_scale <- pc_scale_var/sum(pc_scale_var)
plot(cumsum(pve_scale), xlab="Principal Component (standardized)", ylab ="Cumulative Proportion of Variance Explained",ylim=c(0,1),type="b")
###################################
pc_noscale_var <- (pc_noscale$sdev^2)
pve_noscale <- pc_noscale_var/sum(pc_noscale_var)
plot(cumsum(pve_noscale), xlab="Principal Component (non-standardized)", ylab ="Cumulative Proportion of Variance Explained",ylim=c(0,1),type="b")
``` 

After PCA, we plotted out the cumulatice proportion of variance explained graph of the PCA test. The normalized graph shows that when we use 4 components here it will capture 90 percent of the variance of the data. On the other hand, for the non-normalized graph it shows only one component can capture almost 100 percent of the variance of the data which is unexpected. We think this is maybe because our variable are all closely related because their unit are the area of a house. 

```{r PCR, include=F}
#install.packages("pls")
loadPkg("pls")
```
```{r}
pcr_scale_fit=pcr(price~.,data=house_pca,scale=TRUE,validation ="CV")
summary(pcr_scale_fit)
pcr_noscale_fit=pcr(price~.,data=house_pca,scale=FALSE,validation ="CV")
summary(pcr_noscale_fit)
validationplot(pcr_scale_fit,val.type = "R2")
validationplot(pcr_noscale_fit,val.type = "R2")
```

After using PCA test, we plan to use principal components regression model to fit our data to see if the model is improved compared with linear model we previously used. We look at the results and find if we only need to use 3 components to capture 90 percent of the feature variable variance and 48 percent of the prediction variable. We also run a validation plot for this PCR model and we see the r2 number is around 50 percent when we use 2 components. It mean using these 2 components and this model explains 50 percent of our data.

```{r}
house$yr_renovated[house$yr_renovated>0] <- 1
#str(house)
house$view <- as.numeric(house$view)
house$floors <-as.numeric(house$floors)
house$grade <- as.numeric(house$grade)
house$waterfront  <- as.numeric(house$waterfront )
house$condition  <- as.numeric(house$condition )
house$yr_renovated <- as.numeric(house$yr_renovated)
house$zipcode <- as.numeric(house$zipcode)
house$bedrooms <- as.numeric(house$bedrooms)
house$bathrooms <- as.numeric(house$bathrooms)
```

### PCA/PCR With Factor

This time we apply the PCA and PCR with factor variable included. We would like to see if there is any improvment in this new model.

```{r PCA_with_factor, warning=F}
#subset the needed numeric value
house_pca <- subset(house, select = c(price, bedrooms, bathrooms, floors, waterfront, view, condition, grade, sqft_living, sqft_lot, sqft_above, sqft_basement, sqft_living15, sqft_lot15, yr_built, yr_renovated))
#do pca
pc_scale <- prcomp(house_pca , scale =TRUE)
pc_noscale <- prcomp(house_pca , scale =FALSE)
# pc_scale
# pc_noscale
biplot(pc_scale, scale = 0)
biplot(pc_noscale, scale = 0)
```

Although technicly we should not scale our data when we are using factor by using hamming distance, we still want to see if there is any difference before and after when we scale our data.

```{r}
#Let us plot the cumulation of variance using the sd
pc_scale_var <- (pc_scale$sdev^2)
pve_scale <- pc_scale_var/sum(pc_scale_var)
plot(cumsum(pve_scale), xlab="Principal Component (standardized)", ylab ="Cumulative Proportion of Variance Explained",ylim=c(0,1),type="b")
###################################
pc_noscale_var <- (pc_noscale$sdev^2)
pve_noscale <- pc_noscale_var/sum(pc_noscale_var)
plot(cumsum(pve_noscale), xlab="Principal Component (non-standardized)", ylab ="Cumulative Proportion of Variance Explained",ylim=c(0,1),type="b")
``` 

In this model, we see using 10 component we can capture 90 percent of variance of the data. However, if we not scale the data at all, only one variable will caputure almost 100 percent of the variance.

```{r PCR2}
pcr_scale_fit=pcr(price~.,data=house_pca,scale=TRUE,validation ="CV")
summary(pcr_scale_fit)
pcr_noscale_fit=pcr(price~.,data=house_pca,scale=FALSE,validation ="CV")
summary(pcr_noscale_fit)
validationplot(pcr_scale_fit,val.type = "R2")
validationplot(pcr_noscale_fit,val.type = "R2")
```

We did see improvment in this time, when we use 10 components, 93 percent of the x variable has been explained and 62 percent of the price variable has been captured which was 52 percent in the without factor case above. For the unscaled PCR data, we can see the variance can always be captured around 90 percent. From the validation graph wecan see, there is a huge jump in r2 number when we start to choose more than 3 components in our model, the peak r2 value in this case is over 60 percent. By using the principal component regression, it reduced dimension of our data so easily and we do get some improvment for our model in this case.

```{r}
#change everything back to factors
house$view <- as.factor(house$view)
house$floors <-as.factor(house$floors)
house$grade <- as.factor(house$grade)
house$waterfront  <- as.factor(house$waterfront )
house$condition  <- as.factor(house$condition )
house$yr_renovated <- as.factor(house$yr_renovated)
house$zipcode <- as.factor(house$zipcode)
house$bedrooms <- as.factor(house$bedrooms)
house$bathrooms <- as.factor(house$bathrooms)
house$lat <- as.factor(house$lat)
house$long <- as.factor(house$long)
```


# Chapter 6 Model Evaluations

## K-fold Cross Validation
K-fold cross validation is used so that we can better compare the linear models we have. The order of this cross validation match with what we did before with the linear models, and we also add in backward selection and stepwise selection to do feature selections. By doing a 10 fold cross validation, we want to find out if the last linear model we did is the best one among them.

#### CV on Model 1
```{r, include=F}
loadPkg("caret")
```
```{r}
# Define training control
set.seed(123) 
train.control <- trainControl(method = "cv", number = 10)
model <- train(price ~ grade + view + condition + waterfront + bedrooms + bathrooms, data=house, method = "lm", trControl = train.control)
# Summarize the results
print(model)
```

From the result, we can see that the R^2 is 0.576, which means 57.6% of price is being explained by the x variables, which contain the categorical variables that we think is relevant to price. We have a high RMSE and a high MAE score, which mean that this model might not be a good one, so we move on to the next model, and see if quantitative variables will help with creating a better model.

#### CV on Model 2
```{r,warning=F}
set.seed(123)
model2 <- train(price ~ sqft_lot + sqft_living + sqft_above + sqft_living15 + sqft_lot15, data = house, method = "lm",
               trControl = train.control)
# Summarize the results
print(model2)
```

Unfortunately, based on the results we have, the R^2 value decrease from 0.576 to 0.484, when we change the variables to all quantitative variables. The RMSE value and the MAE also increase a lot, which means that this is not a good model for prediction.

#### CV on Model 3
```{r,warning=F}
set.seed(123)
model3 <- train(price ~ yr_built + yr_renovated, data=house, method = "lm",
               trControl = train.control)
# Summarize the results
print(model3)
```

For model 3, this is definetly not a good model because the R^2 value is only 0.0228, which means only about 2% of price is being explained by the "year built" and "year renovated". The RMSE and MAE values also increase from model 2 to model 3, thus, we believe this is not a good model to consider.

#### CV on Full Model
```{r,warning=F}
set.seed(123)
modelfull <- train(price ~ bedrooms + bathrooms + floors + grade + view + condition + waterfront + sqft_living + sqft_lot + sqft_above + sqft_living15 + sqft_lot15 + yr_built + yr_renovated + sqft_basement, data=house, method = "lm",
               trControl = train.control)

# Summarize the results
print(modelfull)
```

Then, we move to the full model excluding the zipcode, longitude and latitude variables, because these three variables have too many levels, and this will take too long for the model to run and they are not useful for predicting the price. We now have a R^2 value of 0.657, which means 65.7% of price is being explained by the x variables, and this increase in R^2 value make sense because the value should increase as we increase the variables in the model, and the RMSE and MAE values are the smallest values that we contain so far. However, considering the model might overfit the data, and the multicollinearity inside the model, we wanna do a backward selection and a stepwise selection to do feature selecting.


#### CV on Full Model with Backward Selection
```{r,warning=F}
set.seed(123)
model4 <- train(price ~ bedrooms + bathrooms + floors + grade + view + condition + waterfront + sqft_living + sqft_lot + sqft_above + sqft_living15 + sqft_lot15 + yr_built + yr_renovated + sqft_basement, data=house, method = "leapBackward", tuneGrid = data.frame(nvmax = 1:15),
               trControl = train.control)
# Summarize the results
print(model4)
```

For backward selection, it suggests us to use to model with the most variables we have, which means run a full model, since the RMSE will be the smallest. However, a full model is likely to overfit the data.

#### CV on Full Model with Stepwise Selection
```{r, warning=F}
set.seed(123)
# Train the model
model5 <- train(price ~ bedrooms + bathrooms + floors + grade + view + condition + waterfront + sqft_living + sqft_lot + sqft_above + sqft_living15 + sqft_lot15 + yr_built + yr_renovated + sqft_basement, data=house, method = "leapSeq", trace = FALSE, tuneGrid = data.frame(nvmax = 1:15),
               trControl = train.control)
# Summarize the results
print(model5)
```

From the stepwise selections, R told us to use the model with 14 variables, because this way the RMSE will be the smallest. 

```{r, include=F}
options(max.print = 1000000)

summary(model5$finalModel)
```

By doing a subset selection, the results show the significancy of each variable in every model (this part takes too many spaces, therefore not including in the html file). We can consider excluding variables without the asterick because this mean the variables are not significant. Compare the result with the linear model we did before, we can consider excluding "sqft_lot".

#### CV on a feature selected model using stepwise selection 
```{r, warning=F}
set.seed(123)
modelf <- train(price ~ bedrooms + bathrooms + floors + grade + view + condition + waterfront + sqft_living + sqft_above + sqft_living15 + sqft_lot15 + yr_built + yr_renovated + sqft_basement, data = house, method = "lm",trControl = train.control)
# Summarize the results
print(modelf)
```

As the result shown above, the R^2 is 0.657 which is the same with the full model, and the RMSE only increase a little, and MAE decreases, therefore, we can consider this model is better than the full model.

#### CV on Model 5
```{r, warning=F}
set.seed(123)
model6 <- train(price ~ view + bedrooms + bathrooms + sqft_living + sqft_lot + waterfront + condition + grade + yr_built + yr_renovated + sqft_living15 + sqft_lot15 + floors, data = house, method = "lm",trControl = train.control)
# Summarize the results
print(model6)
```

We then did the feature selections based on the significancy in the linear models and research. The results show that this model has R^2 of 0.656, RMSE of 168793 and MAE of 121316. The R^2 value is high and the RMSE and MAE values don't increase a lot, compare to the models before. 

#### Table for Cross Validation
```{r}
modelcom <- matrix(c(187560,0.576,136801, 206680,0.484,151435, 284537,0.0228,208884, 168630,0.657,120982, 168637,0.657,120981, 168793,0.656,121316),ncol=3,byrow=TRUE)
options("scipen" = 100, "digits" = 2)
colnames(modelcom) <- c("RMSE","RSquared","MAE")
rownames(modelcom) <- c("Model 1","Model 2","Model 3", "Full Model", "Model based on Stepwise Selection", "Significant feature Model")
modelcom <- as.table(modelcom)
modelcom
```

Based on the table, we can see the model based on stepwise selection is the best model that we obtain so far. The model contains 14 variables, which matches the result with PCA/PCR that a model with 14 variables will fully explain y variable when scaled. The model also has a high R^2 value of 0.657, which means 65.7% of price is being explained by the x variables, and the RMSE and MAE values don't increase a lot compare to the full model. We don't consider the full model because there is a chance that we might overfit the data. We don't consider the last model because this model contains 13 variables, it doesn't fully explained the y variable based on the PCA/PCR, and the RMSE and MAE values also increase when we deduct variables from the full model. Therefore, the model based on feature selection is the best model that we attain so far.

# Conclusion
To conclude, we mainly focus on the predictive linear model for this dataset, and we want to use the variables we have to predict the housing price in King County. After building multiple linear models, we used PCA/PCR and Cross Validation to evaluate the models we built, and used Backward/Stepwise selections to do feature selections and managed to find the best linear model we can have so far. The final model we want to use is a linear model with 14 variables, which is the one we mention above. For future direction, we could build regression tree models to do features selections, also consider reducing the levels of zipcode, longitude and latitude, so that we can include them into the models to better predict the housing price.

# Reference

King County, Washington. (2019, September 21). Retrieved December 9, 2019, from https://en.wikipedia.org/wiki/King_County,_Washington.

