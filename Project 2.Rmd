---
title: "Project 2: House Sales in King County, USA"
author: "Spencer Stucky, Cheng Zeng, Wenyu Zeng, Chao Zhou"
date: "11/20/2019"
output:
  html_document:
    toc: yes
    toc_depth: 4
    fig.align: "center"
    toc_float: yes
    theme: readable
  pdf_document:
    toc: yes
    toc_depth: '4'
---

# R Markdown

```{r setup, include=FALSE}
# include=T, eval=T, echo=T, results='hide'/'asis',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
knitr::opts_chunk$set(warning = F, results = F, message = F)
# knitr::opts_chunk$set(echo = TRUE)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
```

```{r basicfcn, include=F}
# can add quietly=T option to the require() function
loadPkg = function(x) { if (!require(x,character.only=T, quietly =T)) { install.packages(x,dep=T,repos="http://cran.us.r-project.org"); if(!require(x,character.only=T)) stop("Package not found") } }
```


```{r, echo=F}
house <- read.csv('kc_house_data.csv')
head(house)

str(house)
# house <- na.omit(house) 
summary(house)


```



```{r, include=F}
loadPkg("dplyr")
loadPkg("tidyr")

house$yr_renovated[house$yr_renovated>0] <- 1
```


```{r, echo=F}
# house = house[-2]

house$view <- as.factor(house$view)
house$grade <- as.factor(house$grade)
house$waterfront  <- as.factor(house$waterfront )
house$condition  <- as.factor(house$condition )
house$yr_renovated <- as.factor(house$yr_renovated)
house$zipcode <- as.factor(house$zipcode)
house$bedrooms <- as.factor(house$bedrooms)
house$bathrooms <- as.factor(house$bathrooms)
house$lat <- as.factor(house$lat)
house$long <- as.factor(house$long)

str(house)
head(house)
```


```{r corr, echo=FALSE}
# plot(price ~ view + bedrooms + bathrooms + sqft_living + sqft_lot + floors + waterfront + condition + grade + sqft_above + 
#                 sqft_basement + yr_built + yr_renovated + sqft_living15 + sqft_lot15, data=house)
houseaov= aov(price ~ view + bedrooms + bathrooms + sqft_living + sqft_lot + floors + waterfront + condition + grade + sqft_above + 
                sqft_basement + yr_built + yr_renovated + sqft_living15 + sqft_lot15, data=house)
summary(houseaov)
```
```{r}
# change to histogram 
boxplot(price ~ view, main="Price vs View", data=house, col = rainbow(12) )
boxplot(price ~ waterfront, main="Price vs Waterfront", data=house, col = rainbow(12) )
boxplot(price ~ condition, main="Price vs Condition", data=house, col = rainbow(12) )
boxplot(price ~ grade, main="Price vs grade", data=house, col = rainbow(12) )
boxplot(price ~ yr_renovated, main="Price vs Renovated", data=house, col = rainbow(12))
boxplot(price ~ bathrooms, main="Price vs Renovated", data=house, col = rainbow(12))
boxplot(price ~ bedrooms, main="Price vs Renovated", data=house, col = rainbow(12))
```

```{r}
# loadPkg("corrplot")
# corrplot(housecor)
# corrplot(housecor, method = "square") # try "circle", "square", "ellipse", "number", "shade", "color", "pie"
# corrplot(housecor, method = "number")
# corrplot(housecor, method = "number", type="upper")
# corrplot.mixed(housecor)
```

#EDA

```{r , results='markup', collapse=F}
#change variables to fator level
house$view <- as.factor(house$view)
house$grade <- as.factor(house$grade)
house$condition <- as.factor(house$condition)
house$yr_renovated <- as.factor(house$yr_renovated)
house$waterfront <- as.factor(house$waterfront)


```




# Smart Question
What features are associated with housing prices for King County, WA?

# Linear Model

```{r model 1, results='markup', collapse=F}
loadPkg("faraway")
#model for categorical variables that describe house attributes and ranking
model_1 <- lm(price ~ grade + view + condition + waterfront, data=house)
summary(model_1)
coef(model_1)
confint(model_1)
vif(model_1)

plot(model_1)
```
In this model, we first wanted to look at the categorical variables for house attributes and ranking. R2 is 59% thus model explains 59% of data. All p values are highly signficant at .001 level. All categorical variables, grade, view, condition, waterfront are positively correlated with price and drive up price as they increase.

```{r model 2, results='markup', collapse=F}
#model on distance and size of house in sq ft and neighboring house sizes using numeric variables exluding sqft basement
model_2 <- lm(price ~ sqft_living + sqft_above + sqft_living15 + sqft_lot15, data=house)
summary(model_2)
coef(model_2)
confint(model_2)
vif(model_2)

plot(model_2)
```

Idea behind this model was to look at sq footage and size of house and surrounding neighborhood to determine if they effect price in some way. Also to look at how these sq ft variables may be collinear and relate to one another. Sqft basement is NA and doesnt work for model. Sqft lot is not signficant. Took out these variables for imporved model.Some moderately high VIFs for sqft living and sqft above - most likely some collinearity there. R2 is 50% thus model explains 50% of results seen in data. All p values are significant. Sqft living and Sq ft living 15 are positively correlated with price while sqft above and sqft lot 15 are negatively correlated with price. 


```{r model 3, results='markup', collapse=F}
#recode renovation variable
house$yr_renovated[house$yr_renovated>0] <- 1
head(house)
#model on the time and place of house built and renovated
model_3 <- lm(price ~ yr_built + yr_renovated + zipcode, data=house)
summary(model_3)
coef(model_3)
confint(model_3)
vif(model_3)

plot(model_3)

```{r model 4, results='markup', collapse=F}
#model with all variables
model_4 <- lm(price ~ bedrooms + bathrooms + floors + grade + view + condition + waterfront + sqft_living + sqft_lot + sqft_above + yr_built + yr_renovated + zipcode, data=house)
summary(model_4)
coef(model_4)
confint(model_4)
vif(model_4)

plot(model_4)
```
This model used all variables except the variables on surrounding houses (15). Sq ft basement taken out b/c NA results. R2 is 68%. P values are significant and bedrooms, sqft lot, sqft above, yr built are negatively correlated with price. High VIFs for sqft living and sqft above indicate multicollinearity but nothing too extreme. 

```{r model 5, results='markup', collapse=F}
head(house)
#model with all known significant variables but removed sqft above, sqft basement b/c not significant
model_5 <- lm(price ~ view + bedrooms + bathrooms + sqft_living + sqft_lot + waterfront + condition + grade + yr_built + yr_renovated + sqft_living15 + sqft_lot15 + lat + long + floors + zipcode, data=house)
summary(model_5)
coef(model_5)
confint(model_5)
vif(model_5)

plot(model_5)
```
R2 is about 73% thus model explains 73% of results in data. Removed sqft above and sqft basement b/c not significant. Upper ends of ... All p values are highly significant at the .001 level. A few notable results: as bedrooms increase price decreases, yr built is also negatively correlated with price, as is sq ft of surrounding lots, and zipcode. A moderately high VIF at 5 for sqft living - not high enough for concern.

```{r modelanova, results='markup', collapse=F}
anova(model_1,model_2,model_3,model_4,model_5)
```






### Linear Regression
#### Full Model
```{r}

testlm <- lm(price ~view + bedrooms + bathrooms + sqft_living + sqft_lot + waterfront + condition + grade + sqft_above + sqft_basement + yr_built + yr_renovated + sqft_living15 + sqft_lot15 + lat + long + floors + zipcode, data = house)
summary(testlm)

```



#### Adjusted Linear Model
```{r}
testlm2 <- lm(price ~ view + bedrooms + bathrooms + sqft_living + sqft_lot + waterfront + condition + grade + yr_built + yr_renovated + sqft_living15 + sqft_lot15 + lat + long + floors + zipcode, data = house)
summary(testlm2)
```
### PCA/PCR Without Factor
```{r PCA_without_factor}
#subset the needed numeric value
house_pca <- subset(house, select = c(price, sqft_living, sqft_lot, sqft_above, sqft_basement, sqft_living15, sqft_lot15))
#do pca
pc_scale <- prcomp(house_pca , scale =TRUE)
pc_noscale <- prcomp(house_pca , scale =FALSE)
summary(pc_scale)
summary(pc_noscale)

# pc_noscale
#biplot(pc_scale, scale = 0)
#biplot(pc_noscale, scale = 0)
```
```{r}
#Let us plot the cumulation of variance using the sd
pc_scale_var <- (pc_scale$sdev^2)
pve_scale <- pc_scale_var/sum(pc_scale_var)
plot(cumsum(pve_scale), xlab="Principal Component (standardized)", ylab ="Cumulative Proportion of Variance Explained",ylim=c(0,1),type="b")
###################################
pc_noscale_var <- (pc_noscale$sdev^2)
pve_noscale <- pc_noscale_var/sum(pc_noscale_var)
plot(cumsum(pve_noscale), xlab="Principal Component (non-standardized)", ylab ="Cumulative Proportion of Variance Explained",ylim=c(0,1),type="b")
``` 
```{r PCR}
#install.packages("pls")
loadPkg("pls")
pcr_scale_fit=pcr(price~.,data=house_pca,scale=TRUE,validation ="CV")
summary(pcr_scale_fit)

pcr_noscale_fit=pcr(price~.,data=house_pca,scale=FALSE,validation ="CV")
summary(pcr_noscale_fit)
validationplot(pcr_scale_fit,val.type = "MSEP")
validationplot(pcr_noscale_fit,val.type = "MSEP")
```
```{r}

house$yr_renovated[house$yr_renovated>0] <- 1
str(house)
```

### PCA/PCR With Factor
```{r PCA_with_factor}
#subset the needed numeric value
house_pca <- subset(house, select = c(price, bedrooms, bathrooms, floors, waterfront, view, condition, grade, sqft_living, sqft_lot, sqft_above, sqft_basement, sqft_living15, sqft_lot15, yr_built, yr_renovated))
#do pca
pc_scale <- prcomp(house_pca , scale =TRUE)
pc_noscale <- prcomp(house_pca , scale =FALSE)
# pc_scale
# pc_noscale
#biplot(pc_scale, scale = 0)
#biplot(pc_noscale, scale = 0)
```
```{r}
#Let us plot the cumulation of variance using the sd
pc_scale_var <- (pc_scale$sdev^2)
pve_scale <- pc_scale_var/sum(pc_scale_var)
plot(cumsum(pve_scale), xlab="Principal Component (standardized)", ylab ="Cumulative Proportion of Variance Explained",ylim=c(0,1),type="b")
###################################
pc_noscale_var <- (pc_noscale$sdev^2)
pve_noscale <- pc_noscale_var/sum(pc_noscale_var)
plot(cumsum(pve_noscale), xlab="Principal Component (non-standardized)", ylab ="Cumulative Proportion of Variance Explained",ylim=c(0,1),type="b")
``` 
```{r PCR}
#install.packages("pls")
loadPkg("pls")
pcr_scale_fit=pcr(price~.,data=house_pca,scale=TRUE,validation ="CV")
summary(pcr_scale_fit)

pcr_noscale_fit=pcr(price~.,data=house_pca,scale=FALSE,validation ="CV")
summary(pcr_noscale_fit)
validationplot(pcr_scale_fit,val.type = "R2")
validationplot(pcr_noscale_fit,val.type = "R2")
```
Idea behind this model was to look at yr built, yr renovated, and location of housing to see if they determined something about price. All are highly signficant at .001 level. Year built and Yr renovated were positively correlated with price, with yr built increasing price by 924. R2 is very low at 2.5% thus model does not do a good job of explaining results in data. 




### K-fold Cross Validation
#### Spencer model 1
```{r, warning=F}
loadPkg("caret")
# Define training control
set.seed(123) 
train.control <- trainControl(method = "cv", number = 10)

model <- train(price ~ grade + view + condition + waterfront, data = house, method = "lm", trControl = train.control)
# Summarize the results
print(model)
```

#### Spencer model 2
```{r,warning=F}
set.seed(123)
model2 <- train(price ~ sqft_living + sqft_lot + sqft_above + sqft_basement, data = house, method = "lm",
               trControl = train.control)
# Summarize the results
print(model2)
```

#### Spencer model 3
```{r,warning=F}
set.seed(123)
model3 <- train(price ~ yr_built + yr_renovated + zipcode, data = house, method = "lm",
               trControl = train.control)
# Summarize the results
print(model3)
```

#### Spencer model 4
```{r,warning=F}
set.seed(123)
model4 <- train(price ~ bedrooms + bathrooms + floors + grade + view + condition + waterfront + sqft_living + sqft_lot + sqft_above + yr_built + yr_renovated + zipcode, data = house, method = "leapForward",
               trControl = train.control)
# Summarize the results
print(model4)
```

#### Full Model
```{r, warning=F}
set.seed(123)
# Train the model
model5 <- train(price ~view + bedrooms + bathrooms + sqft_living + sqft_lot + waterfront + condition + grade + sqft_above + sqft_basement + yr_built + yr_renovated + sqft_living15 + sqft_lot15 + lat + long + floors + zipcode, data = house, method = "leapSeq", tuneGrid = data.frame(nvmax = 1:18),
               trControl = train.control)
# Summarize the results
print(model5)
```

#### Feature Selection based on the full model
```{r, warning=F}
set.seed(123)
model6 <- train(price ~ view + bedrooms + bathrooms + sqft_living + sqft_lot + waterfront + condition + grade + yr_built + yr_renovated + sqft_living15 + sqft_lot15 + lat + long + floors + zipcode, data = house, method = "lm",trControl = train.control)
# Summarize the results
print(model6)
```

#### Table for Cross Validation
```{r}
modelcom <- matrix(c(236009,0.586,153783, 260958,0.494,173416, 362133,0.024,231985, 208763,0.675,135348, 191991,0.726,119763, 191973,0.726,119731),ncol=3,byrow=TRUE)
options("scipen" = 100, "digits" = 2)
colnames(modelcom) <- c("RMSE","RSquared","MAE")
rownames(modelcom) <- c("Model 1","Model 2","Model 3", "Model 4", "Full Model", "Significant feature Model")
modelcom <- as.table(modelcom)
modelcom
```



### KNN
```{r}
set.seed(1)
house_data_train_rows = sample(1:nrow(house),     #<- from 1 to the number of 
                                                     #   rows in the data set
                              round(0.8 * nrow(house), 0),  #<- multiply the 
                                                                #   number of rows
                                                                #   by 0.8 and round
                                                                #   the decimals
                              replace = FALSE)       #<- don't replace the numbers

# Let's check to make sure we have 80% of the rows. 
length(house_data_train_rows) / nrow(house)

house_data_train = house[house_data_train_rows, ]  #<- select the rows identified in
                                                     #   the bank_data_train_rows data
house_data_test = house[-house_data_train_rows, ]  #<- select the rows that weren't
                                                     #   identified in the
                                                     #   bank_data_train_rows data

# Check the number of rows in each set.
head(house_data_train)
nrow(house_data_test)

```
Train the classifier

```{r}
# Let's train the classifier for k = 3. 
# Install the "class" package that we'll use to run kNN.
# Take some time to learn about all its functionality.
# install.packages("class") 
loadPkg("class")

# k-Nearest Neighbor is a randomized algorithm, so make sure to
# use set.seed() to make your results repeatable.
set.seed(1)
house_3NN = knn(train = house_data_train[, c("price", "bedrooms", "bathrooms", "sqft_living", "sqft_lot", "sqft_above", "sqft_living15", "yr_built", "lat", "long")],  #<- training set cases
               test = house_data_test[, c("price", "bedrooms", "bathrooms", "sqft_living", "sqft_lot", "sqft_above", "sqft_living15", "yr_built", "lat", "long")],    #<- test set cases
               cl = house_data_train[, "view"],                         #<- category for true classification
               k = 3) #,                                                    #<- number of neighbors considered
               # use.all = TRUE)                                            #<- control ties between class assignments
                                                                            #   If true, all distances equal to the kth 
                                                                            #   largest are included

# View the output.
str(house_3NN)
length(house_3NN)
table(house_3NN)
```
ANOVA shows us that the latter three models are statistically significant and are better at explaining the data. However, last model has a high RSS which indicates it is not a close fit model. 

#```{r plot, results='markup', collapse=F}
loadPkg("ggplot2")
loadPkg("directlabels")
loadPkg("dplyr")
library(directlabels)
ggplot(house, aes(x=price, y=sqft_living)) + 
    geom_line(aes(color=price, group=price), size = 1) +   # create the line by nacis code
    ggtitle("Line Plot of Continuing Denial Percentage by Year")+
    geom_point(aes(color=price), size=3) + 
    xlab("Year")+ ylab("Percentage(%)") + 
    ggsave("condenline.png", width=12, height=9)    
#```



```{r}
# How does the kNN classification compare to the true class?
# Let's take a look at the confusion matrix by combining the 
# predictions from bank_3NN to the original data set.
kNN_res = table(house_3NN,
                house_data_test$`view`)
kNN_res
sum(kNN_res)  #<- the total is all the test examples

# Select the true positives and true negatives by selecting
# only the cells where the row and column names are the same.
kNN_res[row(kNN_res) == col(kNN_res)]

# Calculate the accuracy rate by dividing the correct classifications
# by the total number of classifications.
kNN_acc = sum(kNN_res[row(kNN_res) == col(kNN_res)]) / sum(kNN_res)
kNN_acc
```

### K-MEAN
```{r, warning=F}
loadPkg("dplyr")
input <- house %>% select("bedrooms", "bathrooms", "sqft_living", "sqft_lot", "sqft_above", "sqft_living15", "yr_built", "lat", "long")
kmeans(input, centers = 3, nstart = 20)
```
```{r}
wssplot <- function(data, nc=15, seed=123){
               wss <- (nrow(data)-1)*sum(apply(data,2,var))
               for (i in 2:nc){
                    set.seed(seed)
                    wss[i] <- sum(kmeans(data, centers=i)$withinss)}
                plot(1:nc, wss, type="b", xlab="Number of groups",
                     ylab="Sum of squares within a group")}

wssplot(input, nc = 20)
```

```{r}
k2 <- kmeans(input, centers = 4, nstart = 20)
```

```{r}
loadPkg("factoextra")
fviz_cluster(k2, data = input)
```

